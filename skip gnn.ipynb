{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e82bbda-c026-4c74-92c4-158ef22246ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2940713-489e-46bf-9ade-39bd18dbc85c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "'Node2Vec' requires either the 'pyg-lib' or 'torch-cluster' package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m edge_index = torch.cat([edge_index, edge_index.flip(\u001b[32m0\u001b[39m)], dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Make it bidirectional\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 3. Apply Node2Vec (Skip-gram-style embeddings)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m node2vec = \u001b[43mNode2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwalk_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwalks_per_node\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_negative_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m embed_optimizer = torch.optim.SparseAdam(\u001b[38;5;28mlist\u001b[39m(node2vec.parameters()), lr=\u001b[32m0.01\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_node2vec\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch_geometric\\nn\\models\\node2vec.py:67\u001b[39m, in \u001b[36mNode2Vec.__init__\u001b[39m\u001b[34m(self, edge_index, embedding_dim, walk_length, context_size, walks_per_node, p, q, num_negative_samples, num_nodes, sparse)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m p == \u001b[32m1.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m q == \u001b[32m1.0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m                           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrequires either the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyg-lib\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m                           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtorch-cluster\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     71\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m                           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrequires the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtorch-cluster\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: 'Node2Vec' requires either the 'pyg-lib' or 'torch-cluster' package"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv, Node2Vec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "# 1. Load and prepare the dataset\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "X = df.drop(columns=[\"Outcome\"]).values\n",
    "y = df[\"Outcome\"].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# 2. Build k-NN graph and extract edge index\n",
    "A = kneighbors_graph(X, n_neighbors=10, mode='connectivity')\n",
    "G = nx.from_scipy_sparse_array(A)\n",
    "edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().contiguous()\n",
    "edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)  # Make it bidirectional\n",
    "\n",
    "# 3. Apply Node2Vec (Skip-gram-style embeddings)\n",
    "node2vec = Node2Vec(edge_index=edge_index, embedding_dim=64, walk_length=20,\n",
    "                    context_size=10, walks_per_node=10, num_negative_samples=1,\n",
    "                    p=1, q=1, sparse=True)\n",
    "\n",
    "embed_optimizer = torch.optim.SparseAdam(list(node2vec.parameters()), lr=0.01)\n",
    "\n",
    "def train_node2vec():\n",
    "    node2vec.train()\n",
    "    total_loss = 0\n",
    "    for _ in range(100):\n",
    "        embed_optimizer.zero_grad()\n",
    "        loss = node2vec.loss()\n",
    "        loss.backward()\n",
    "        embed_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / 100\n",
    "\n",
    "print(\"Training Node2Vec embeddings...\")\n",
    "loss = train_node2vec()\n",
    "print(f\"Node2Vec training loss: {loss:.4f}\")\n",
    "\n",
    "# 4. Replace X with Node2Vec embeddings\n",
    "data = Data(x=node2vec.embedding.weight.detach(), edge_index=edge_index, y=y_tensor)\n",
    "\n",
    "# 5. Define the GNN model using GraphSAGE\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, output_dim)\n",
    "        self.fc = Linear(output_dim, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# 6. Train the GNN model\n",
    "model = GNN(input_dim=64, hidden_dim=32, output_dim=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(300):\n",
    "    loss = train()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "# 7. Evaluate the model\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        acc = (pred == data.y).sum().item() / len(data.y)\n",
    "        print(f'Accuracy: {acc:.4f}')\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be71ea0-39e9-4680-ac3b-f582ea898587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
